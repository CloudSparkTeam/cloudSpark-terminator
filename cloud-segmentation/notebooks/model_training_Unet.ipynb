{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"KQEdkoBrZB3iNhBNZDyx\")\n",
    "project = rf.workspace(\"api5\").project(\"segmentacao-de-nuvens\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"png-mask-semantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Caminho da pasta onde estão as imagens e máscaras misturadas\n",
    "source_dir = 'Segmentação-de-Nuvens-1/train'\n",
    "images_dir = os.path.join(source_dir, 'images')\n",
    "masks_dir = os.path.join(source_dir, 'masks')\n",
    "\n",
    "# Crie as pastas 'images' e 'masks' se elas não existirem\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "os.makedirs(masks_dir, exist_ok=True)\n",
    "\n",
    "# Itere sobre os arquivos na pasta de origem\n",
    "for filename in os.listdir(source_dir):\n",
    "    # Ignore pastas para evitar mover as novas pastas 'images' e 'masks' dentro delas mesmas\n",
    "    file_path = os.path.join(source_dir, filename)\n",
    "    if os.path.isdir(file_path):\n",
    "        continue\n",
    "\n",
    "    # Verifique se o arquivo é uma máscara\n",
    "    if '_mask' in filename:\n",
    "        # Mova as máscaras para a pasta 'masks'\n",
    "        shutil.move(file_path, os.path.join(masks_dir, filename))\n",
    "    else:\n",
    "        # Presume que todos os outros arquivos são imagens\n",
    "        shutil.move(file_path, os.path.join(images_dir, filename))\n",
    "\n",
    "print('Arquivos organizados nas pastas \"images/\" e \"masks/\".')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Treinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(images_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.images[idx]\n",
    "        image_path = os.path.join(self.images_dir, image_name)\n",
    "\n",
    "        # Substituir a extensão da imagem para encontrar a máscara correspondente\n",
    "        mask_name = image_name.replace('.jpg', '_mask.png')  # ou .png, dependendo do seu formato\n",
    "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "\n",
    "        # Carregar a imagem e a máscara\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convertido para escala de cinza\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina os diretórios das imagens e máscaras\n",
    "images_dir = 'Segmentação-de-Nuvens-1/train/images'  # ajuste conforme necessário\n",
    "masks_dir = 'Segmentação-de-Nuvens-1/train/masks'      # ajuste conforme necessário\n",
    "\n",
    "# Defina as transformações para imagem e máscara\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Ajuste conforme necessário\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Crie o dataset e o dataloader\n",
    "dataset = SegmentationDataset(images_dir, masks_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, masks):\n",
    "    \"\"\"Função para mostrar um lote de imagens e máscaras.\"\"\"\n",
    "    fig, axs = plt.subplots(2, len(images), figsize=(12, 6))\n",
    "    for i in range(len(images)):\n",
    "        # Mostrar a imagem\n",
    "        axs[0, i].imshow(images[i].permute(1, 2, 0))  # Permuta as dimensões para matplotlib\n",
    "        axs[0, i].set_title('Image')\n",
    "        axs[0, i].axis('off')  # Oculta os eixos\n",
    "\n",
    "        # Mostrar a máscara\n",
    "        mask = masks[i].squeeze()  # Remove a dimensão extra\n",
    "        axs[1, i].imshow(mask, cmap='gray')\n",
    "        axs[1, i].set_title('Mask')\n",
    "        axs[1, i].axis('off')  # Oculta os eixos\n",
    "    plt.tight_layout()  # Ajusta os espaços entre os subplots\n",
    "    plt.show()\n",
    "\n",
    "# Pega um lote de dados\n",
    "for images, masks in dataloader:\n",
    "    show_images(images, masks)\n",
    "    break  # Exibe apenas o primeiro lote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.masks = os.listdir(mask_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Converter a máscara para escala de cinza\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        # Adicione um print para verificar a forma das máscaras\n",
    "        print(f\"Image shape: {image.shape}, Mask shape: {mask.shape}\")\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Criando o DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = CustomDataset(images_dir, masks_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Iterando sobre o DataLoader para verificar as formas das imagens e máscaras\n",
    "for images, masks in dataloader:\n",
    "    print(f\"Batch of images shape: {images.shape}, Batch of masks shape: {masks.shape}\")\n",
    "    break  # Remova ou mantenha isso para imprimir apenas o primeiro lote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Adicione mais camadas de codificação conforme necessário\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.decoder(x1)\n",
    "        return x2\n",
    "\n",
    "# Inicialize o modelo\n",
    "model = UNet(in_channels=3, out_channels=1)  # 1 para máscara binária\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Você pode ajustar a taxa de aprendizado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Importe a biblioteca tqdm\n",
    "\n",
    "num_epochs = 100  # Defina o número de épocas\n",
    "for epoch in range(num_epochs):\n",
    "    # Use tqdm para mostrar a barra de progresso\n",
    "    for images, masks in tqdm(dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False):\n",
    "        optimizer.zero_grad()  # Zerar os gradientes\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, masks)  # Cálculo da perda\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Atualizar os pesos\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "torch.save(model.state_dict(), 'unet_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
